In the context of a discrete Tetris environment, the phenomenon you described can be explained as follows:

1. Training Phase:
- Masked-Agent: During the training phase, the masked-agent might perform better initially because it has a form of randomness injected into its actions. In the discrete Tetris environment, this randomness could be achieved by randomly choosing to ignore or mask some possible moves, forcing the agent to explore different actions and potentially discover better strategies. This exploration can lead to the agent finding more optimal placements for the tetrominoes and clearing lines more effectively.

- Unmasked-Agent: On the other hand, the unmasked-agent is not subjected to this exploration noise and might struggle to explore the action space efficiently during training. It may not consider certain moves that could lead to better outcomes, and as a result, it might converge to a suboptimal policy during the training phase.

2. Evaluation Phase:
- Masked-Agent: During evaluation, the exploration noise is removed or reduced, and the masked-agent tends to exploit the policy it has learned during training. In the discrete Tetris environment, this means the agent will now make decisions based on its learned strategy without any additional randomness. However, in Tetris, randomness can be beneficial in dealing with the inherent uncertainty in the game, such as the arrival of tetrominoes, so the absence of exploration noise can lead to suboptimal performance during evaluation.

- Unmasked-Agent: The unmasked-agent, which had more time to train without exploration noise, has potentially learned a more accurate representation of the optimal policy for playing Tetris. As a result, it performs better during evaluation compared to the masked-agent, which might have become overly reliant on exploration noise during training.

To address this issue and improve the performance of the agents, various exploration strategies can be used during training. In the context of discrete Tetris, exploration techniques could involve introducing a small probability of randomly choosing actions (e.g., random moves or rotations) even for the unmasked-agent during training. This allows the agent to explore the action space more effectively while still benefiting from the learned policy during evaluation.

Additionally, learning algorithms like Deep Q Networks (DQN) can benefit from fine-tuning the exploration-exploitation trade-off by adjusting parameters like epsilon in an epsilon-greedy exploration strategy. This way, the agent can strike a better balance between exploring new moves and exploiting its learned policy effectively.

In summary, in a discrete Tetris environment, understanding and managing the exploration-exploitation trade-off is crucial for training agents that perform well both during training and evaluation. Properly balanced exploration strategies can help the agent discover effective tactics while avoiding overfitting to exploration noise, leading to improved performance in the long run.
